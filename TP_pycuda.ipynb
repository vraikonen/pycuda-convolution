{"cells":[{"cell_type":"markdown","metadata":{"id":"fADg7cWG7lKy"},"source":["# Practical session: first steps with PyCuda\n"]},{"cell_type":"markdown","metadata":{"id":"9wKCzO6l75dU"},"source":["In this assignment, you will learn about CUDA through the PyCuda language. Pycuda is a Python interface to the CUDA language, and more specifically a wrapper around C functions."]},{"cell_type":"markdown","metadata":{"id":"Go5bRHk6_ux5"},"source":["Online documentation is available at this address: : https://documen.tician.de/pycuda/"]},{"cell_type":"markdown","metadata":{"id":"6VcT-FpEAA-8"},"source":["Before you start, you need to enable GPU support in Collab. To do this go to :\n"," - Edit->Notebook Settings\n","\n","and activate the GPU (Hardware acceleration)."]},{"cell_type":"markdown","metadata":{"id":"6nQFvQLU7tUD"},"source":["The following cell allows you to install PyCuda in your Collab work environment"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":154708,"status":"ok","timestamp":1705874986751,"user":{"displayName":"Nikola Vracevic","userId":"09899277253294975207"},"user_tz":-60},"id":"Nk7XtrIe10h7","outputId":"1c192941-4b1c-4400-bf2a-e85643953a9d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pycuda\n","  Downloading pycuda-2024.1.tar.gz (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting pytools>=2011.2 (from pycuda)\n","  Downloading pytools-2023.1.1-py2.py3-none-any.whl (70 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from pycuda) (1.4.4)\n","Collecting mako (from pycuda)\n","  Downloading Mako-1.3.0-py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.1.0)\n","Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.5.0)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from mako->pycuda) (2.1.3)\n","Building wheels for collected packages: pycuda\n","  Building wheel for pycuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycuda: filename=pycuda-2024.1-cp310-cp310-linux_x86_64.whl size=661205 sha256=ad71ea2aa6f50d9d683b7e2b87bc8eca9e87f9047948eed756c71cdecd659aa7\n","  Stored in directory: /root/.cache/pip/wheels/12/34/d2/9a349255a4eca3a486d82c79d21e138ce2ccd90f414d9d72b8\n","Successfully built pycuda\n","Installing collected packages: pytools, mako, pycuda\n","Successfully installed mako-1.3.0 pycuda-2024.1 pytools-2023.1.1\n"]}],"source":["!pip install pycuda"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6869,"status":"ok","timestamp":1705874993578,"user":{"displayName":"Nikola Vracevic","userId":"09899277253294975207"},"user_tz":-60},"id":"A5zz2_UWC0VQ","outputId":"aa7eeef3-99e8-4ef5-ccbf-0db8a416f80a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting scikit-cuda\n","  Downloading scikit_cuda-0.5.3-py2.py3-none-any.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.8/114.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: mako>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from scikit-cuda) (1.3.0)\n","Requirement already satisfied: numpy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-cuda) (1.23.5)\n","Requirement already satisfied: pycuda>=2016.1 in /usr/local/lib/python3.10/dist-packages (from scikit-cuda) (2024.1)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from mako>=1.0.1->scikit-cuda) (2.1.3)\n","Requirement already satisfied: pytools>=2011.2 in /usr/local/lib/python3.10/dist-packages (from pycuda>=2016.1->scikit-cuda) (2023.1.1)\n","Requirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from pycuda>=2016.1->scikit-cuda) (1.4.4)\n","Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda>=2016.1->scikit-cuda) (4.1.0)\n","Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda>=2016.1->scikit-cuda) (4.5.0)\n","Installing collected packages: scikit-cuda\n","Successfully installed scikit-cuda-0.5.3\n"]}],"source":["pip install scikit-cuda"]},{"cell_type":"markdown","metadata":{"id":"RYT6XBu2A2NU"},"source":["Here is a simple example (a kernel that multiplies term by term the elements of a table), from the PyCuda documentation. Before starting, test its execution and check the output of the program."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1101,"status":"ok","timestamp":1705874995092,"user":{"displayName":"Nikola Vracevic","userId":"09899277253294975207"},"user_tz":-60},"id":"warPD8gS1-bP","outputId":"a7c80efa-ef63-4d95-b1e3-b79ae02786d1"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"]}],"source":["import pycuda.autoinit\n","import pycuda.driver as drv\n","import numpy\n","\n","from pycuda.compiler import SourceModule\n","mod = SourceModule(\"\"\"\n","__global__ void multiply_them(float *dest, float *a, float *b)\n","{\n","  const int i = threadIdx.x;\n","  dest[i] = a[i] * b[i];\n","}\n","\"\"\")\n","\n","multiply_them = mod.get_function(\"multiply_them\")\n","\n","a = numpy.random.randn(400).astype(numpy.float32)\n","b = numpy.random.randn(400).astype(numpy.float32)\n","\n","dest = numpy.zeros_like(a)\n","multiply_them(\n","        drv.Out(dest), drv.In(a), drv.In(b),\n","        block=(400,1,1), grid=(1,1))\n","\n","print(dest-a*b)"]},{"cell_type":"markdown","metadata":{"id":"mx9rfyHx-Mj9"},"source":["Try to design the same code with `GPUarray`\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1705874995092,"user":{"displayName":"Nikola Vracevic","userId":"09899277253294975207"},"user_tz":-60},"id":"HFDrujiw-Mj-","outputId":"2894d3f5-3d19-4847-aeff-5333947e1621"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"]}],"source":["from pycuda.gpuarray import GPUArray\n","import numpy as np\n","import time\n","\n","\n","mod = SourceModule(\"\"\"\n","__global__ void multiply_them(float *dest, float *a, float *b)\n","{\n","  const int i = threadIdx.x;\n","  dest[i] = a[i] * b[i];\n","}\n","\"\"\")\n","\n","multiply_them = mod.get_function(\"multiply_them\")\n","\n","a = np.random.randn(400).astype(np.float32)\n","b = np.random.randn(400).astype(np.float32)\n","\n","dest_gpu = GPUArray(a.shape, np.float32)\n","a_gpu = GPUArray(a.shape, np.float32)\n","b_gpu = GPUArray(b.shape, np.float32)\n","\n","dest_gpu.set(a * b)\n","a_gpu.set(a)\n","b_gpu.set(b)\n","\n","multiply_them(dest_gpu, a_gpu, b_gpu, block=(400, 1, 1), grid=(1, 1))\n","\n","dest_gpu_result = np.empty_like(a)\n","dest_gpu.get(dest_gpu_result)\n","\n","print(dest_gpu_result - a * b)\n"]},{"cell_type":"markdown","metadata":{"id":"D0EsLOsHBCSN"},"source":["\n","# First part\n","In the first part of this session, we will see how to define a very simple kernel, allowing to invert the contents of a table.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xYgCEI2s2gii"},"outputs":[],"source":["N = 100000\n","my_tab = numpy.arange(N)\n","# print(my_tab)\n","# my_tab[-1]"]},{"cell_type":"markdown","metadata":{"id":"N9h6UHUaYtAG"},"source":["First code your kernel function\n"]},{"cell_type":"markdown","metadata":{"id":"EakhTEUlZMFN"},"source":["Then test it:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":540,"status":"ok","timestamp":1705874995630,"user":{"displayName":"Nikola Vracevic","userId":"09899277253294975207"},"user_tz":-60},"id":"meZuFZAQZNdb","outputId":"c9fb25fb-d461-42b2-eeff-bc8a4507c3e4"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/google/colab/_variable_inspector.py:27: UserWarning: module in out-of-thread context could not be cleaned up\n","  globals().clear()\n"]},{"name":"stdout","output_type":"stream","text":["Original Array:\n","[    0     1     2 ... 99997 99998 99999]\n","\n","Inverted Array:\n","[99999 99998 99997 ...     2     1     0]\n"]}],"source":["N = 100000\n","my_tab = numpy.arange(N, dtype=numpy.int32)\n","\n","mod = SourceModule(\"\"\"\n","__global__ void invert_tab(int *my_tab, int N) {\n","    int idx = threadIdx.x + blockDim.x * blockIdx.x;\n","\n","    if (idx < N) {\n","        my_tab[idx] = N - 1 - my_tab[idx];\n","    }\n","}\n","\"\"\")\n","\n","invert_tab = mod.get_function(\"invert_tab\")\n","\n","# GPU memory\n","my_tab_gpu = drv.mem_alloc(my_tab.nbytes)\n","\n","# Transfer my_tab to GPU\n","drv.memcpy_htod(my_tab_gpu, my_tab)\n","\n","# Configure kernel\n","block_size = 256\n","grid_size = (N + block_size - 1) // block_size\n","invert_tab(my_tab_gpu, numpy.int32(N), block=(block_size, 1, 1), grid=(grid_size, 1))\n","\n","# Copy the result back to CPU\n","drv.memcpy_dtoh(my_tab, my_tab_gpu)\n","\n","# Print two arrays\n","print(\"Original Array:\")\n","print(numpy.arange(N))\n","print(\"\\nInverted Array:\")\n","print(my_tab)\n","\n","# Test\n","expected_result = numpy.arange(N, dtype=numpy.int32)[::-1]\n","assert (my_tab == expected_result).all(), \"Test failed!\""]},{"cell_type":"markdown","metadata":{"id":"XuCjMlJ98XjR"},"source":["# Second part\n","On the same basis, now write a kernel that takes a matrix of data of size NxN, and calculates a vector of size N which is the sum of each line of the matrix.\n","\n","You will take a grid of size (4,4,1)\n","\n","Test."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BwEBClXg88G4"},"outputs":[],"source":["N = 4\n","matrix = numpy.random.randn(N, N).astype(numpy.float32)\n","\n","mod = SourceModule(\"\"\"\n","__global__ void sum_rows(float *matrix, float *sum_vector, int N) {\n","    int row = blockIdx.y * blockDim.y + threadIdx.y;\n","\n","    if (row < N) {\n","        float sum = 0.0f;\n","        for (int i = 0; i < N; ++i) {\n","            sum += matrix[row * N + i];\n","        }\n","        sum_vector[row] = sum;\n","    }\n","}\n","\"\"\")\n","\n","sum_rows = mod.get_function(\"sum_rows\")\n","\n","# GPU memory\n","matrix_gpu = drv.mem_alloc(matrix.nbytes)\n","sum_vector_gpu = drv.mem_alloc(N * numpy.float32().itemsize)\n","\n","# Transfer data to GPU\n","drv.memcpy_htod(matrix_gpu, matrix)\n","\n","# Configure kernel\n","block_size = (1, 4, 1)\n","grid_size = (1, (N + block_size[1] - 1) // block_size[1], 1)\n","sum_rows(matrix_gpu, sum_vector_gpu, numpy.int32(N), block=block_size, grid=grid_size)\n","\n","# Copy the result back to CPU\n","sum_vector = numpy.empty_like(matrix[:, 0], dtype=numpy.float32)\n","drv.memcpy_dtoh(sum_vector, sum_vector_gpu)\n","\n","# Test\n","expected_sum_vector = numpy.sum(matrix, axis=1)\n","assert (sum_vector == expected_sum_vector).all(), f\"Assertion failed: {sum_vector} vs {expected_sum_vector}\""]},{"cell_type":"markdown","metadata":{"id":"vCoGhADz-MkA"},"source":["# Third part\n","Try to implement a kernel that computes the product of two square matrices, as seen in the course. Compare its execution time with the same  product written in numpy. For what size of the matrix do you get acceleration ?\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":588,"status":"ok","timestamp":1705874996524,"user":{"displayName":"Nikola Vracevic","userId":"09899277253294975207"},"user_tz":-60},"id":"oEQf7mUB-MkA","outputId":"0284f07f-e4f3-4763-fb67-c012e5e21d69"},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU Result:\n","[[ 11.871438    8.589899   27.330284  ...  -9.542024   12.873382\n","   12.727498 ]\n"," [ 21.745056   14.524875  -10.145751  ...  -2.4736772 -55.797756\n","   25.716566 ]\n"," [-29.202597   -5.1413164 -10.481479  ... -27.710413   16.680696\n","   22.223158 ]\n"," ...\n"," [-24.316147   58.34827   -16.907825  ... -40.165874   -1.0559348\n","   -2.329887 ]\n"," [ 13.075979  -55.709743   18.962326  ... -21.105322   18.362955\n","  -15.534467 ]\n"," [  7.102917   13.199413  -47.24878   ... -55.80871    -4.2112136\n","   20.100468 ]]\n","\n","CPU Result:\n","[[ 11.871475    8.589907   27.330263  ...  -9.5420265  12.873375\n","   12.727499 ]\n"," [ 21.745062   14.524878  -10.145751  ...  -2.4736814 -55.797737\n","   25.716564 ]\n"," [-29.202595   -5.1413136 -10.481472  ... -27.710423   16.680693\n","   22.223145 ]\n"," ...\n"," [-24.316147   58.348267  -16.907822  ... -40.1659     -1.0559444\n","   -2.3298893]\n"," [ 13.07598   -55.709747   18.96231   ... -21.105326   18.362963\n","  -15.534463 ]\n"," [  7.1029134  13.199425  -47.248802  ... -55.80867    -4.211193\n","   20.100458 ]]\n","Matrix size: 1000x1000\n","GPU Time: 0.00724 seconds\n","CPU Time: 0.03469 seconds\n","Speedup: 4.79x\n"]}],"source":["def matrix_multiply_cpu(a, b):\n","    return numpy.dot(a, b)\n","\n","N = 1000\n","\n","# Generate random matrices\n","matrix_a = numpy.random.randn(N, N).astype(numpy.float32)\n","matrix_b = numpy.random.randn(N, N).astype(numpy.float32)\n","\n","# PyCUDA kernel for matrix multiplication\n","mod = SourceModule(\"\"\"\n","__global__ void matrix_multiply(float *a, float *b, float *c, int N) {\n","    int row = blockIdx.y * blockDim.y + threadIdx.y;\n","    int col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if (row < N && col < N) {\n","        float sum = 0.0f;\n","        for (int k = 0; k < N; ++k) {\n","            sum += a[row * N + k] * b[k * N + col];\n","        }\n","        c[row * N + col] = sum;\n","    }\n","}\n","\"\"\")\n","\n","matrix_multiply_gpu = mod.get_function(\"matrix_multiply\")\n","\n","# Allocate GPU memory\n","matrix_a_gpu = drv.mem_alloc(matrix_a.nbytes)\n","matrix_b_gpu = drv.mem_alloc(matrix_b.nbytes)\n","result_gpu = drv.mem_alloc(matrix_a.nbytes)\n","\n","# Transfer data to GPU\n","drv.memcpy_htod(matrix_a_gpu, matrix_a)\n","drv.memcpy_htod(matrix_b_gpu, matrix_b)\n","\n","# Configure kernel\n","block_size = (16, 16, 1)\n","grid_size = ((N + block_size[0] - 1) // block_size[0], (N + block_size[1] - 1) // block_size[1], 1)\n","\n","start_time = time.time()\n","matrix_multiply_gpu(matrix_a_gpu, matrix_b_gpu, result_gpu, numpy.int32(N), block=block_size, grid=grid_size)\n","drv.Context.synchronize()\n","gpu_time = time.time() - start_time\n","\n","# Copy the result back to CPU\n","result_gpu_matrix = numpy.empty_like(matrix_a)\n","drv.memcpy_dtoh(result_gpu_matrix, result_gpu)\n","\n","# Perform CPU matrix multiplication\n","start_time = time.time()\n","result_cpu_matrix = matrix_multiply_cpu(matrix_a, matrix_b)\n","cpu_time = time.time() - start_time\n","\n","print(\"GPU Result:\")\n","print(result_gpu_matrix)\n","print(\"\\nCPU Result:\")\n","print(result_cpu_matrix)\n","\n","\n","epsilon = 1e-3  # Tolerance level\n","assert numpy.allclose(result_gpu_matrix, result_cpu_matrix, rtol=epsilon, atol=epsilon), \"Results do not match!\"\n","\n","\n","\n","print(f\"Matrix size: {N}x{N}\")\n","print(f\"GPU Time: {gpu_time:.5f} seconds\")\n","print(f\"CPU Time: {cpu_time:.5f} seconds\")\n","print(f\"Speedup: {cpu_time / gpu_time:.2f}x\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
